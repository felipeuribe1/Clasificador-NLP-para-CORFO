{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototipo de clasificación de reclamos CMF\n",
    "\n",
    "##### Prototipo para postulación a etapa 1 de \"Reto de Innovación de Interés Público SupTech para la Supervisión de Conducta en Mercados Financieros\"\n",
    "\n",
    "El siguiente documento contiene el modelo propuesto para resolver la clasificación binaria de reclamos de valores y seguros, junto con el procesamiento de los 100 reclamos sin etiqueta. El modelo ya fue entrenado con los 1000 reclamos con etiqueta, por lo que en este documento se importan los parámetros ya ajustados. Para mayor información sobre el entrenamiento, favor reisar documento \"Entrenamiento de Prototipo\".\n",
    "\n",
    "\n",
    "Autores:\n",
    "* Bastian Ermann Rodríguez <<bastian@ermannb.com>>\n",
    "* Pablo Uribe Pizarro <<pablo.uribe@student.ecp.fr>>\n",
    "* Felipe Uribe Pizarro <<felipe.uribe.pizarro@gmail.com>>\n",
    "\n",
    "08/02/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de packages\n",
    "\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de datos\n",
    "\n",
    "En primera instancia, se carga archivo excel de 100 reclamos sin clasificación, en formato pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conforme a lo indicado en OFORD N°XX, SGD: N°XX de fecha 12 de noviembre del 2014, envío documentación adjunta, específicamente, certificado en el cual mi jefatura indica que el día 27 de Junio del presente año, el suscrito se encontraba de servicio de guardia, cumpliendo funciones propias de un Funcionario Público de la PDI., Lo anterior conforme al motivo de fuerza mayor enunciado en el Art. 16, número 2 de las condiciones generales de la póliza. Haciendo presente que en el Art N°45 del Código Civil se indica; Se llama fuerza mayor o caso fortuito el imprevisto a que no es posible resistir, como un naufragio, un terremoto, el apresamiento de enemigos, los actos de autoridad ejercidos por un funcionario público, etc. En mi situación por ser funcionario público.\n",
      "Con respecto a que al momento de la denuncia se indicó que el vehículo siniestrado ya había aparecido, debo indicar que dicha información fue recepcionada telefónicamente durante se realizaba denuncia ante personal de Carabineros de Chile.\n",
      "En relación al denuncio a la compañía que se realizó el día 28, fue debido a que el suscrito realizó incesantes llamados telefónicos a dicha compañía para denunciar el siniestro no teniendo resultados. Conforme lo anterior el día 28 se logró realizar el denuncio vía telefónica asignándome al liquidador XX, a quien comencé a llamar el mismo día, no teniendo respuesta hasta el día lunes 30 del mes de junio, momentos en que me indica que debía completar un formulario por el concepto de robo, Formulario que envió vía correo electrónico el día 09 de julio, el cual fue devuelto por el suscrito el día 10 de julio vía correo electrónico y personalmente en la aseguradora Magallanes, adjuntando además fotografías del vehículo siniestrado a petición del liquidador. Haciendo presente que el liquidador el día 28 de julio me llamó telefónicamente para que rectificara lo manifestado en el formulario de robo que llenó el suscrito.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "file_sclas =pd.read_excel(cwd+r'/articles-40220_recurso_1/reclamos_20201221_sin_clas.xlsx')\n",
    "file_sclas.set_index('CASO_ID')\n",
    "\n",
    "\n",
    "# Ejemplo de un reclamo sin procesamiento\n",
    "\n",
    "print(file_sclas['DESCRIPCION_CIUDADANO'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos\n",
    "\n",
    "Tras importar los reclamos, se realiza un preprocesamiento de los mismos previo a ser ingresados al modelo. El objetivo es dejar sólo las palabras más relevantes en el texto a analizar, en un formato estándar. Este paso contempla lo siguiente:\n",
    "\n",
    "1. Remoción de caracteres fuera del abecedario español (puntuación, números, paréntesis, guiones, etc).\n",
    "2. Tokenización, es decir, transformar cada reclamo en una lista de palabras (strings).\n",
    "3. Remoción de \"stopwords\" (palabras comunes en español, que no agregan mayor información para la clasificación).\n",
    "4. Estandarización de palabras, transformando todas las letras a minúsculas.\n",
    "\n",
    "Se imprimen ejemplos de \"stopwords\" a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('spanish')[:10])\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    text = re.sub(r'[^A-Za-zÑñÁáÉéÍíÓóÚú]', ' ', text)\n",
    "    \n",
    "    tokens = text.split()\n",
    "    \n",
    "    stop_words = stopwords.words('spanish')\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if (token not in stop_words and len(token) > 1)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se aplica el preprocesamiento a las columnas 'DESCRIPCION_CIUDADANO' y 'PETICION_CIUDADANO'\n",
    "\n",
    "file_sclas['DESCRIPCION_CIUDADANO']=file_sclas['DESCRIPCION_CIUDADANO'].apply(preprocess)\n",
    "file_sclas['PETICION_CIUDADANO']=file_sclas['PETICION_CIUDADANO'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conforme', 'indicado', 'oford', 'xx', 'sgd', 'xx', 'fecha', 'noviembre', 'envío', 'documentación', 'adjunta', 'específicamente', 'certificado', 'jefatura', 'indica', 'día', 'junio', 'presente', 'año', 'suscrito', 'encontraba', 'servicio', 'guardia', 'cumpliendo', 'funciones', 'propias', 'funcionario', 'público', 'pdi', 'anterior', 'conforme', 'motivo', 'fuerza', 'mayor', 'enunciado', 'art', 'número', 'condiciones', 'generales', 'póliza', 'haciendo', 'presente', 'art', 'código', 'civil', 'indica', 'llama', 'fuerza', 'mayor', 'caso', 'fortuito', 'imprevisto', 'posible', 'resistir', 'naufragio', 'terremoto', 'apresamiento', 'enemigos', 'actos', 'autoridad', 'ejercidos', 'funcionario', 'público', 'etc', 'situación', 'ser', 'funcionario', 'público', 'respecto', 'momento', 'denuncia', 'indicó', 'vehículo', 'siniestrado', 'aparecido', 'debo', 'indicar', 'dicha', 'información', 'recepcionada', 'telefónicamente', 'realizaba', 'denuncia', 'personal', 'carabineros', 'chile', 'relación', 'denuncio', 'compañía', 'realizó', 'día', 'debido', 'suscrito', 'realizó', 'incesantes', 'llamados', 'telefónicos', 'dicha', 'compañía', 'denunciar', 'siniestro', 'resultados', 'conforme', 'anterior', 'día', 'logró', 'realizar', 'denuncio', 'vía', 'telefónica', 'asignándome', 'liquidador', 'xx', 'comencé', 'llamar', 'mismo', 'día', 'respuesta', 'día', 'lunes', 'mes', 'junio', 'momentos', 'indica', 'debía', 'completar', 'formulario', 'concepto', 'robo', 'formulario', 'envió', 'vía', 'correo', 'electrónico', 'día', 'julio', 'devuelto', 'suscrito', 'día', 'julio', 'vía', 'correo', 'electrónico', 'personalmente', 'aseguradora', 'magallanes', 'adjuntando', 'además', 'fotografías', 'vehículo', 'siniestrado', 'petición', 'liquidador', 'haciendo', 'presente', 'liquidador', 'día', 'julio', 'llamó', 'telefónicamente', 'rectificara', 'manifestado', 'formulario', 'robo', 'llenó', 'suscrito']\n"
     ]
    }
   ],
   "source": [
    "#Ejemplo de reclamo preprocesado\n",
    "\n",
    "print(file_sclas['DESCRIPCION_CIUDADANO'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulario de entrenamiento\n",
    "\n",
    "Para poder ingresar los reclamos al modelo, previamente se deben codificar las palabras a IDs, lo que se conoce como un vocabulario. El vocabulario es generado en la fase de entrenamiento del modelo, utilizando las palabras de los 1000 reclamos con etiqueta.\n",
    "\n",
    "A continuación, se filtran los reclamos sin clasificación con el vocabulario generado en entrenamiento, para luego traducir cada palabra a su respectivo ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se carga vocabulario\n",
    "\n",
    "import json\n",
    "with open('vocab_lstm_v4.json') as json_file: \n",
    "    vocab = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se filtran los reclamos, para que contengan sólo palabras dentro del vocabulario de entrenamiento del modelo\n",
    "\n",
    "def filter_vocab(text,vocab):\n",
    "\n",
    "    filtered_text = [word for word in text if (word in vocab)]\n",
    "    return filtered_text\n",
    "\n",
    "file_sclas['DESCRIPCION_CIUDADANO']=file_sclas['DESCRIPCION_CIUDADANO'].apply(filter_vocab,vocab = vocab)\n",
    "file_sclas['PETICION_CIUDADANO']=file_sclas['PETICION_CIUDADANO'].apply(filter_vocab,vocab = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se unen columnas 'DESCRIPCION_CIUDADANO' y 'PETICION_CIUDADANO' en un sólo gran string a ser procesado por el modelo\n",
    "\n",
    "X_sclas=(file_sclas['DESCRIPCION_CIUDADANO']+file_sclas['PETICION_CIUDADANO']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se codifican palabras en sus respectivos ID's\n",
    "\n",
    "X_ints = []\n",
    "for text in X_sclas:\n",
    "    X_ints.append([vocab[word] for word in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 1465, 3072, 11, 6061, 11, 115, 470, 2187, 278, 351, 3301, 271, 4, 67, 812, 482, 69, 930, 1481, 957, 5857, 2401, 5570, 3154, 2138, 1782, 603, 18, 920, 304, 5093, 77, 20, 21, 10, 1140, 482, 5093, 3138, 2341, 4, 1398, 304, 677, 1514, 2309, 4605, 2783, 2138, 1782, 1757, 580, 451, 2138, 1782, 319, 123, 837, 784, 406, 3175, 5195, 196, 1132, 540, 50, 109, 2789, 2771, 837, 595, 1166, 462, 1983, 2312, 34, 1507, 67, 597, 930, 1507, 512, 4542, 540, 34, 802, 354, 1917, 18, 603, 67, 1054, 2312, 411, 412, 1056, 11, 2298, 897, 421, 67, 249, 67, 2677, 295, 812, 1635, 4, 259, 1275, 489, 1008, 190, 489, 1869, 411, 2, 1219, 67, 909, 1487, 930, 67, 909, 411, 2, 1219, 2251, 355, 1212, 1821, 30, 2183, 406, 3175, 1490, 1056, 1140, 482, 1056, 67, 909, 3797, 2789, 5608, 489, 190, 930, 3888, 406, 3175, 1281, 53, 54, 703, 222, 2020, 115]\n"
     ]
    }
   ],
   "source": [
    "#Ejemplo de texto codificado\n",
    "\n",
    "print(X_ints[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estandarización de número de palabras en textos\n",
    "\n",
    "Para que el modelo pueda trabajar con textos que poseen distinto número de palabras, se estandariza cada uno considerando el máximo número de palabras para un reclamo en el entrenamiento (231). Para ello, se rellenan todos los textos con ceros (palabra \"vacía\") hasta llegar al largo estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(text_ints, seq_length):\n",
    "    \n",
    "    features = np.zeros((len(text_ints), seq_length), dtype=int)\n",
    "\n",
    "    for i, row in enumerate(text_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0   18 1465 3072   11 6061   11  115  470 2187  278  351 3301\n",
      "   271    4   67  812  482   69  930 1481  957 5857 2401 5570 3154 2138\n",
      "  1782  603   18  920  304 5093   77   20   21   10 1140  482 5093 3138\n",
      "  2341    4 1398  304  677 1514 2309 4605 2783 2138 1782 1757  580  451\n",
      "  2138 1782  319  123  837  784  406 3175 5195  196 1132  540   50  109\n",
      "  2789 2771  837  595 1166  462 1983 2312   34 1507   67  597  930 1507\n",
      "   512 4542  540   34  802  354 1917   18  603   67 1054 2312  411  412\n",
      "  1056   11 2298  897  421   67  249   67 2677  295  812 1635    4  259\n",
      "  1275  489 1008  190  489 1869  411    2 1219   67  909 1487  930   67\n",
      "   909  411    2 1219 2251  355 1212 1821   30 2183  406 3175 1490 1056\n",
      "  1140  482 1056   67  909 3797 2789 5608  489  190  930 3888  406 3175\n",
      "  1281   53   54  703  222 2020  115]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "seq_length = 231\n",
    "\n",
    "features = pad_features(X_ints, seq_length=seq_length)\n",
    "\n",
    "assert len(features)==len(X_ints), \"Número de filas no coinciden.\"\n",
    "assert len(features[0])==seq_length, \"Número de columnas no coinciden.\"\n",
    "\n",
    "# Ejemplo de texto estandarizado\n",
    "\n",
    "print(features[:1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de clasificación\n",
    "\n",
    "La base del modelo corresponde a un tipo de red neuronal recurrente, llamada LSTM. Esta clase de red neuronal es capaz de procesar información de una secuencia de palabras dentro de un texto. La arquitectura de la red se ilustra a continuación.\n",
    "\n",
    "<img src='lstm.png' width=\"600\" height=\"600\">\n",
    "\n",
    "En primer lugar, se utiliza una capa de embedding, que realiza una representación de cada ID del vocabulario en un vector. Esta capa tiene dos finalidades: representar semánticamente la palabra (vectores cercanos poseen contextos similares), y reducir la dimensión de entrada a la red LSTM.\n",
    "\n",
    "A continuación, los embeddings se pasan a las celdas LSTM. Esta capa genera dos salidas: un output que se pasa a la capa siguiente, que actúa como clasificador propiamente tal; y un segundo valor que es pasado a la misma LSTM, que cumple el rol de ser la \"memoria\" de largo plazo de la red. Este último punto es el que permite que las redes LSTM tengan buenos resultados para problemas con secuencias de datos.\n",
    "\n",
    "Finalmente, los outputs de la etapa anterior se pasan a una capa sigmoide. Al tratarse de un problema de clasificación binaria, las clases se representan como 0 (valores) o 1 (seguros). La capa sigmoide entrega un número entre 0 y 1, interpretable como la probabilidad de pertenecer a cada clase. En el esquema se ilustra que se obtiene un único output (al ingresar la útima palabra), ya que se busca que el modelo entrege la clasificación una vez hayan ingresado todas las palabras del reclamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class lstm_classifier(nn.Module):\n",
    "  \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \n",
    "        super(lstm_classifier, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_classifier(\n",
      "  (embedding): Embedding(6327, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=2, batch_first=True, dropout=0.7)\n",
      "  (dropout): Dropout(p=0.7, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Instanciar el modelo con sus hiperparámetros\n",
    "\n",
    "vocab_size = len(vocab)+1 # +1 por la palabra 0 \"vacía\"\n",
    "output_size = 1\n",
    "embedding_dim = 200\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = lstm_classifier(vocab_size, output_size, embedding_dim, hidden_dim, n_layers,drop_prob=0.7)\n",
    "\n",
    "# Se cargan parámetros internos del modelo, obtenidos a través del entrenamiento\n",
    "\n",
    "net.load_state_dict(torch.load('lstm_v4.pth'))\n",
    "\n",
    "# Imprime arquitecura del modelo\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 1., 1., 1.], grad_fn=<RoundBackward>)\n"
     ]
    }
   ],
   "source": [
    "data = torch.from_numpy(features)\n",
    "h = net.init_hidden(len(X_sclas))\n",
    "net.eval()\n",
    "\n",
    "# Se obtiene output de probabilidades de pertenecer a cada clase\n",
    "\n",
    "output, h = net(data, h)\n",
    "    \n",
    "# Se convierten las probabilidades en clases (0 or 1)\n",
    "\n",
    "pred = torch.round(output.squeeze()) \n",
    "\n",
    "# Se muestra output\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Clasificaciones: 100\n",
      "Distribución: 0.68\n"
     ]
    }
   ],
   "source": [
    "# Verificación de número de clasificaciones realizadas\n",
    "\n",
    "print(\"Número de Clasificaciones: \" + str(len(pred)))\n",
    "\n",
    "# Distrubución de clases en la predicción\n",
    "\n",
    "print(\"Distribución: {:.2f}\".format(np.mean(pred.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportación de datos a csv para ser anexados a archivo Excel\n",
    "\n",
    "x_np = pred.detach().numpy()\n",
    "x_df = pd.DataFrame(x_np)\n",
    "x_df.to_csv('lstm_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "A modo de resumen, la metodología de utilización del prototipo para la clasificación de nuevos reclamos es la siguiente:\n",
    "1. Importación de datos\n",
    "2. Preprocesamiento de datos\n",
    "3. Filtro de nuevos reclamos con el vocabulario de entrenamiento\n",
    "4. Estandarización de largo de reclamos\n",
    "5. Intanciamiento de arquitecura del modelo y sus hiperparámetros, e importación de parámetros internos\n",
    "6. Obtención de la predicción del modelo\n",
    "\n",
    "La predicción obtenida a partir del modelo muestra los siguientes valores:\n",
    "* Número de Clasificaciones: 100\n",
    "* Distribución: 68% de reclamos clasificados como 'APIA -Reclamo Seguros ' (32% como 'Reclamo Valores')\n",
    "\n",
    "Por lo tanto, el modelo fue capaz de entregar una clasificación a la totalidad de reclamos nuevos. Se observa que la predicción posee una distribución similar a la observada en el conjunto de 1000 datos de entrenamiento (66% para 'APIA -Reclamo Seguros '), lo cual es una indicación de consistencia del modelo. Sin embargo, pueden existir diferencias con las métricas estimadas en el entrenamiento para la presente clasificación, dado que éste fue realizado con un pequeño número de datos. A medida que el modelo sea entrenado con un mayor volumen de datos, se puede obtener una mayor certeza de que las métricas de entrenamiento representen correctamente su performance con nuevos reclamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
